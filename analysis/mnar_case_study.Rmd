---
title: "mnar_case_study"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# Core
library(tidyverse)   # includes ggplot2, dplyr, tidyr, purrr, readr, etc.
library(janitor)
library(skimr)

# Plot helpers / stats
library(viridis)
library(patchwork)
library(reshape2)   

# Correlation / matrix
library(polycor)
library(Matrix)

# Synthetic generation
library(copula)

# Imputation
library(mice)

# Evaluation / utilities
library(pROC)
library(caTools)
```

## 1. Data Preparation

We load the Kaggle student grading dataset and construct a reduced set of predictors
covering demographics, behavior, and assessments. Letter grades are later mapped to a
binary outcome (passing) for predictive modeling. Figure 1 summarizes heterogeneous
correlations across mixed variable types to guide MNAR scenario design.

```{r}
set.seed(7654321)

source("../R/01_data_preparation.R")

prep <- run_data_preparation(
  csv_path = "../data/students_performance_dataset.csv",
  fig_corr_path = "../figures/fig1_heterogeneous_corr.png"
)

raw  <- prep$raw
data <- prep$data
```

## 2. Synthetic Data Generation

We generate synthetic datasets using a Gaussian copula to preserve dependence across mixed data types
while allowing flexible marginal distributions. Labels are generated by sampling from a logistic model
fit on the seed data. Figure 2 validates that representative variables align well between empirical and
synthetic distributions.

```{r}
source("../R/02_synthetic_generation.R")

syn <- run_synthetic_generation(
  data = data,
  n = 5000,
  seed = 123,
  fig2_path = "../figures/fig2_real_vs_synth.png"
)

synthetic <- syn$synthetic
syn$synth_pass_rate
```

## 3. MNAR Mechanisms
In this study, we induce Missing Not At Random (MNAR) defects by making the probability of missingness depend on the unobserved value itself. This mirrors real reporting behaviors where students may selectively withhold information based on performance or engagement.

We design univariate MNAR scenarios to isolate the effect of losing a single predictor with different relevance to the outcome:

- Final exam score (high correlation): missingness concentrated in the upper tail (RIGHT), reflecting high-performing students withholding grades.
- Participation (moderate correlation): missingness concentrated in the lower tail (LEFT), reflecting disengaged students omitting low participation.
- Study hours (low correlation): missingness concentrated in the lower tail (LEFT), reflecting low-effort students hiding minimal study time.

We also design multivariate MNAR scenarios where several predictors go missing simultaneously to represent broader information loss:

- Academic disengagement (high impact): missingness affects final, midterm, participation simultaneously, driven by the final score mechanism.
- Student burnout (low impact): missingness affects stress, sleep, study simultaneously, driven by the stress mechanism.

Each scenario is tested at missingness intensities of 25%, 40%, 55%, and 70%, allowing us to quantify how performance degradation scales with the amount of informative missingness.

```{r, echo=FALSE}
source("../R/03_mnar_mechanisms.R")

# Use one synthetic dataset as a demonstration
df0 <- synthetic

# Univariate example: final goes missing (RIGHT tail) at 70%
df_uni <- apply_mnar_scenario(df0, scenario = "final", prop = 0.70)

# Multivariate example: academic disengagement at 70%
df_multi <- apply_mnar_scenario(df0, scenario = "academic_disengagement", prop = 0.70)

missing_rate <- function(df) sort(colMeans(is.na(df)), decreasing = TRUE)

list(
  univariate_final_70 = head(missing_rate(df_uni), 6),
  multivariate_academic_70 = head(missing_rate(df_multi), 10)
)
```

## 4. Imputation Methods
We repair MNAR-damaged training data using Predictive Mean Matching (PMM) as a standard imputation baseline under realistic deployment constraints: the outcome variable passing is excluded from the imputation predictor matrix to prevent target leakage.

To explore whether domain knowledge can partially correct MNAR bias, we apply a delta adjustment to PMM-imputed values. Specifically, for entries that were originally missing under MNAR, we shift the imputed values by one standard deviation in either the correct direction (counteracting the MNAR tail mechanism) or the incorrect direction (misdiagnosed mechanism), which quantifies the risk of manual adjustments.

```{r, echo=FALSE}
source("../R/04_imputation_methods.R")

# Example damaged dataset from Section 3 (pick one)
df_mnar <- df_multi  # e.g., academic_disengagement at 70%

# PMM only
df_pmm <- impute_with_strategy(df_mnar, "PMM", exclude_target = TRUE, seed = 1)

# PMM + delta (correct direction example)
# Provide target_vars explicitly for clarity
target_vars <- names(df_mnar)[colSums(is.na(df_mnar)) > 0]
df_delta <- impute_with_strategy(df_mnar, "PMM+DELTA_ADD", target_vars = target_vars, exclude_target = TRUE, seed = 1)

list(
  missing_before = sort(colMeans(is.na(df_mnar)), decreasing = TRUE)[1:6],
  missing_after_pmm = sum(is.na(df_pmm)),
  missing_after_delta = sum(is.na(df_delta))
)
```

## 5. Model and Simulation
```{r, echo=FALSE}
source("../R/05_model_and_simulation.R")

sim_bank_complete <- generate_sim_bank_complete(
  iterations = 500, n = 5000,
  generate_predictors_all = syn$generator$generate_predictors_all,
  attach_passing = syn$generator$attach_passing,
  fit_logit = syn$fit_logit,
  train_df = data,
  seed = 123
)

res <- run_mnar_simulation(
  sim_bank_complete = sim_bank_complete,
  data_for_sim = data_for_sim,
  iterations = 500,
  props_to_test = MNAR_PROPS,
  scenarios_uni = MNAR_UNI_VARS,
  scenarios_multi = MNAR_MULTI_SCENARIOS,
  base_model_formula = base_model_formula
)

head(res)
```

## 6. Evaaluation
We summarize Monte Carlo outputs by averaging performance metrics across iterations for each combination of scenario and missingness intensity. We report three metrics: AUC, accuracy, and misclassification rate (1 âˆ’ accuracy). Results are visualized as degradation curves over missingness intensity and compared across repair strategies. We additionally run paired t-tests on selected conditions (same dataset split per iteration) to assess whether performance differences are statistically significant.

```{r, echo=FALSE}
source("../R/06_evaluation.R")

results_summary <- summarize_results(all_results)
results_long <- to_long_format(results_summary)

# Save main figures
fig_paths <- build_and_save_all_figures(results_long, out_dir = "../figures")

# Optional: run paired tests
tt <- run_paired_tests(all_results, prop = 0.40, scenario = "final")
```